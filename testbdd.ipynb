{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import urllib.robotparser\n",
    "import re\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_extensions = (\n",
    "    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp',\n",
    "    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm', '.mkv',\n",
    "    '.mp3', '.wav', '.ogg', '.flac', '.aac',\n",
    "    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\n",
    "    '.zip', '.rar', '.7z', '.tar', '.gz',\n",
    "    '.exe', '.bin', '.iso', '.dmg'\n",
    ")\n",
    "\n",
    "ignore_keywords = (\n",
    "    'index', 'connexion', 'inscription', 'mailto', 'tel', 'xml',\n",
    "    'login', 'javascript', 'logout', 'register', 'signup', 'user',\n",
    "    'account', 'settings', 'preferences', 'profile', 'admin',\n",
    "    'private', 'dashboard', 'terms', 'privacy', 'policy', 'license',\n",
    "    'captcha', 'auth', 'subscribe', 'unsubscribe', 'download',\n",
    "    'uploads', 'file', 'files', 'attachment', 'comments', 'cgv', 'faq', 'amazon', 'shop', 'snapchat', 'tiktok', 'twitter', 'facebook', 'instagram'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "    dbname='Google_Crawler',  # Nom de votre base de données\n",
    "    user='postgres',     # Votre utilisateur PostgreSQL\n",
    "    password='240305',  # Votre mot de passe PostgreSQL\n",
    "    host='localhost',    # Adresse du serveur PostgreSQL\n",
    "    port='5432' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_url_to_start = input(\"Combien d'url voulez-vous crawler ? \")\n",
    "nb_url_to_start = int(nb_url_to_start)\n",
    "urls = []\n",
    "\n",
    "for i in range(nb_url_to_start):\n",
    "    url = input(\"URL :\")\n",
    "    if url.endswith('/'):\n",
    "        url = url[:-1]\n",
    "    urls.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(urls, depth):\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    if depth == 0:\n",
    "        return 'Done'\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Erreur de connexion\")\n",
    "            continue\n",
    "\n",
    "        content = response.content\n",
    "        str_content = content.decode('utf-8')\n",
    "\n",
    "        soup = BeautifulSoup(str_content, \"html.parser\")\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        # Compter le nombre de mots \n",
    "        words = re.findall(r'\\w+', text)\n",
    "\n",
    "        if detect(text) != 'fr':\n",
    "            print(\"La page n'est pas en français\")\n",
    "            continue\n",
    "\n",
    "        # Ajout a la bdd\n",
    "        try :\n",
    "\n",
    "            query = \"\"\"\n",
    "                INSERT INTO pages (url, content, search_vector, titre, nombre_mots)\n",
    "                VALUES (%s, %s, to_tsvector('french', %s), %s, %s)\n",
    "                ON CONFLICT (url) DO UPDATE\n",
    "                SET content = EXCLUDED.content,\n",
    "                    search_vector = to_tsvector('french', EXCLUDED.content),\n",
    "                    updated_at = CURRENT_TIMESTAMP;\n",
    "                \"\"\"\n",
    "\n",
    "            cursor.execute(query, (url, text, text, soup.title.string if soup.title else '', len(words)))\n",
    "            conn.commit()\n",
    "\n",
    "            query = sql.SQL(\"SELECT id FROM pages WHERE url = %s;\")\n",
    "            cursor.execute(query, (url,))\n",
    "            page_id = cursor.fetchone()[0]\n",
    "    \n",
    "            # Recuperation du chemin racine de l'url\n",
    "            root_url = f'{urlparse(url).scheme}://{urlparse(url).netloc}' \n",
    "            if not root_url.endswith('/'):\n",
    "                root_url += '/'\n",
    "            url_robots =root_url + 'robots.txt'\n",
    "    \n",
    "            # Recuperation du fichier robots.txt\n",
    "            rp = urllib.robotparser.RobotFileParser()\n",
    "            rp.set_url(url_robots)\n",
    "            rp.read()\n",
    "\n",
    "            print(\"Insertion des liens dans la base de données\")\n",
    "            list_url = []\n",
    "            for link in soup.find_all('a' , href=True):\n",
    "                relative_url = link.get('href')\n",
    "                absolute_url = urljoin(url, relative_url)\n",
    "\n",
    "                # Verifier si le lien est autorisé par le robots.txt\n",
    "                if not rp.can_fetch('*', absolute_url):\n",
    "                    print(\"Le lien n'est pas autorisé par le robots.txt\")\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                if (absolute_url == url or \n",
    "                    any(keyword in absolute_url for keyword in ignore_keywords) or \n",
    "                    any(absolute_url.endswith(ext) for ext in ignore_extensions)):\n",
    "                    continue\n",
    "                \n",
    "                query = \"\"\"\n",
    "                INSERT INTO to_crawl (id_url_source,url)\n",
    "                VALUES (%s, %s)\n",
    "                ON CONFLICT (url) DO NOTHING;\n",
    "                \"\"\"\n",
    "                cursor.execute(query, (page_id, absolute_url))\n",
    "\n",
    "                list_url.append(absolute_url)\n",
    "            print(len(list_url))\n",
    "            conn.commit()   \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            conn.rollback()\n",
    "\n",
    "        crawler(list_url,depth-1)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection()\n",
    "cursor = conn.cursor()\n",
    "crawler(urls,3)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler sur les sites deja dans la base en l'enrichissant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import psycopg2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from psycopg2 import sql\n",
    "from psycopg2.pool import SimpleConnectionPool\n",
    "\n",
    "# Configuration de la connexion à la base de données\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'Google_Crawler',\n",
    "    'user': 'postgres',\n",
    "    'password': '240305',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Pool de connexions\n",
    "connection_pool = SimpleConnectionPool(1, 20, **DB_CONFIG)\n",
    "\n",
    "def get_db_connection():\n",
    "    return connection_pool.getconn()\n",
    "\n",
    "def release_db_connection(conn):\n",
    "    connection_pool.putconn(conn)\n",
    "\n",
    "def normalize_url(base_url, link):\n",
    "    \"\"\"Normalize the URL to handle relative URLs.\"\"\"\n",
    "    if link.startswith('http'):\n",
    "        return link\n",
    "    return urljoin(base_url, link)\n",
    "\n",
    "# Fonction pour traiter chaque URL\n",
    "def crawl_url(url):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Erreur de connexion pour l'URL: {url}\")\n",
    "            cursor.execute(\n",
    "                sql.SQL(\"\"\"UPDATE to_crawl SET crawled = TRUE WHERE url = %s;\"\"\"),\n",
    "                (url,)\n",
    "            )\n",
    "            conn.commit()\n",
    "            return\n",
    "\n",
    "        content = response.content\n",
    "        string_content = content.decode('utf-8')\n",
    "\n",
    "        soup = BeautifulSoup(string_content, \"html.parser\")\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        words = re.findall(r'\\w+', text)\n",
    "\n",
    "\n",
    "        if detect(text) != 'fr':\n",
    "            print(f\"La page {url} n'est pas en français\")\n",
    "            cursor.execute(\n",
    "                sql.SQL(\"\"\"UPDATE to_crawl SET crawled = TRUE WHERE url = %s;\"\"\"),\n",
    "                (url,)\n",
    "            )\n",
    "            conn.commit()\n",
    "            return\n",
    "        \n",
    "        title = soup.title.string if soup.title else 'NULL'\n",
    "\n",
    "        cursor.execute(\n",
    "            sql.SQL(\"\"\"INSERT INTO pages (url, content, search_vector, titre, nombre_mots)\n",
    "            VALUES (%s, %s, to_tsvector('french', %s), %s, %s)\n",
    "            ON CONFLICT (url) DO UPDATE\n",
    "            SET content = EXCLUDED.content,\n",
    "                search_vector = to_tsvector('french', EXCLUDED.content),\n",
    "                updated_at = CURRENT_TIMESTAMP;\"\"\"),\n",
    "            (url, text, text, title, len(words))\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "        # Extraire les liens\n",
    "        links = set()\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            full_url = normalize_url(url, link['href'])\n",
    "            if urlparse(full_url).netloc == urlparse(url).netloc:  # interne\n",
    "                links.add((url, full_url))\n",
    "\n",
    "        # Insérer les liens dans la table links\n",
    "        for from_url, to_url in links:\n",
    "            cursor.execute(\n",
    "                sql.SQL(\"\"\"INSERT INTO links (from_page_id, to_page_id)\n",
    "                SELECT p1.id, p2.id\n",
    "                FROM pages p1, pages p2\n",
    "                WHERE p1.url = %s AND p2.url = %s\n",
    "                ON CONFLICT (from_page_id, to_page_id) DO NOTHING;\"\"\"),\n",
    "                (from_url, to_url)\n",
    "            )\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.execute(\n",
    "            sql.SQL(\"\"\"UPDATE to_crawl SET crawled = TRUE WHERE url = %s;\"\"\"),\n",
    "            (url,)\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "    except (psycopg2.Error, requests.RequestException) as e:\n",
    "        print(f\"Erreur pour l'URL {url}: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        release_db_connection(conn)\n",
    "\n",
    "conn = get_db_connection()\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT url FROM to_crawl WHERE crawled = false ORDER BY created_at ASC ;\")\n",
    "urls_to_crawl = cursor.fetchall()\n",
    "nbWorker = 8\n",
    "while True:\n",
    "    time.sleep(1)\n",
    " \n",
    "    if not urls_to_crawl:\n",
    "        print(\"Aucune URL à crawler\")\n",
    "        break\n",
    "\n",
    "    urls = []\n",
    "    for i in range(nbWorker):\n",
    "        if i < len(urls_to_crawl):\n",
    "            urls.append(urls_to_crawl[i][0])\n",
    "\n",
    "    print(urls)\n",
    "\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=nbWorker) as executor:\n",
    "        executor.map(crawl_url, [url for url in urls])\n",
    "\n",
    "    for i in range(nbWorker):\n",
    "        if i < len(urls_to_crawl):\n",
    "            urls_to_crawl.pop(0)\n",
    "    print(\"Fin de l'itération\")\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mettre dans la bdd tous les mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    query = sql.SQL(\"SELECT mot, occurences FROM index_mot where mot = 'montagne'\")\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    occurences = cursor.fetchall()\n",
    "    \n",
    "    if len(occurences) > 0:\n",
    "        occurences = occurences[0][1]\n",
    "\n",
    "        pattern = re.compile(r'\\((\\d+),([^\\)]+)\\)')\n",
    "        matches = pattern.findall(occurences)\n",
    "        matches = [(int(match[0]), match[1]) for match in matches]\n",
    "        # supprimer les anti slash\n",
    "        matches = [(count, url.replace('\\\\','')) for count, url in matches]\n",
    "        matches = [(count, url.replace('\"','')) for count, url in matches]\n",
    "        matches = [(count, url.replace(' ','')) for count, url in matches]\n",
    "\n",
    "        matches.sort(reverse=True)\n",
    "\n",
    "        for match in matches:\n",
    "            print(match[0], match[1])\n",
    "\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(e)\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mettre en place TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try : \n",
    "    print(\"Recherche de la page la plus pertinente\")\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = sql.SQL(\"SELECT COUNT(*) FROM pages\")\n",
    "    cursor.execute(query)\n",
    "    nb_pages = cursor.fetchone()[0]\n",
    "        \n",
    "    query = sql.SQL(\"SELECT mot FROM index_mot ORDER BY mot DESC\")\n",
    "    cursor.execute(query)\n",
    "    mots = cursor.fetchall()\n",
    "    for mot in mots:\n",
    "        print(mot[0])\n",
    "        query = sql.SQL(\"SELECT occurences FROM index_mot WHERE mot = %s\")\n",
    "        cursor.execute(query, (mot[0],))\n",
    "        occurences = cursor.fetchall()\n",
    "\n",
    "    \n",
    "\n",
    "        if len(occurences) > 0:\n",
    "            occurences = occurences[0][0]\n",
    "            pattern = re.compile(r'\\((\\d+),([^\\)]+)\\)')\n",
    "            matches = pattern.findall(occurences)\n",
    "            matches = [(int(match[0]), match[1]) for match in matches]\n",
    "            matches = [(count, url.replace('\\\\','')) for count, url in matches]\n",
    "            matches = [(count, url.replace('\"','')) for count, url in matches]\n",
    "            matches = [(count, url.replace(' ','')) for count, url in matches]\n",
    "            \n",
    "            # calcul du idf\n",
    "            idf = np.log(nb_pages / len(matches))\n",
    "        \n",
    "\n",
    "            # calcul du tf idf\n",
    "            liste_url = []\n",
    "            for url in matches:\n",
    "                cursor.execute(sql.SQL(\"SELECT nombre_mots FROM pages WHERE url = %s\"), (url[1],))\n",
    "                nb_mots = cursor.fetchone()[0]\n",
    "                tf = url[0] / nb_mots\n",
    "                tf_idf = tf * np.log(idf)\n",
    "                liste_url.append((url[1], tf_idf))\n",
    "\n",
    "\n",
    "                # calculer maintenant le PageRank\n",
    "                \n",
    "\n",
    "            liste_url.sort(key=lambda x: x[1], reverse=True)\n",
    "            for url in liste_url:\n",
    "                print(url[0], url[1])\n",
    "            print(\"La page la plus pertinente est : \", liste_url[0][0])\n",
    "            print()\n",
    "            break\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(e)\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import re\n",
    "\n",
    "def get_connection():\n",
    "    # Connexion à la base de données\n",
    "    conn = psycopg2.connect(dbname='Google_Crawler', user='postgres', password='240305', host='localhost', port='5432')\n",
    "    return conn\n",
    "\n",
    "try:\n",
    "    print(\"Recherche de la page la plus pertinente\")\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Récupérer le nombre total de pages\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM pages\")\n",
    "    nb_pages = cursor.fetchone()[0]\n",
    "\n",
    "    # Récupérer les pages et construire le mapping id -> index\n",
    "    cursor.execute(\"SELECT id, url FROM pages\")\n",
    "    pages = cursor.fetchall()\n",
    "    page_ids = {page_id: index for index, (page_id, _) in enumerate(pages)}\n",
    "    urls = {index: url for index, (_, url) in enumerate(pages)}\n",
    "\n",
    "    # Initialiser la matrice de liens\n",
    "    link_matrix = np.zeros((nb_pages, nb_pages))\n",
    "\n",
    "    # Récupérer les liens et remplir la matrice de liens\n",
    "    cursor.execute(\"SELECT from_page_id, to_page_id FROM links\")\n",
    "    links = cursor.fetchall()\n",
    "    for from_page_id, to_page_id in links:\n",
    "        if from_page_id in page_ids and to_page_id in page_ids:\n",
    "            link_matrix[page_ids[from_page_id], page_ids[to_page_id]] = 1\n",
    "\n",
    "    # Normaliser la matrice de liens\n",
    "    out_link_counts = np.sum(link_matrix, axis=1)\n",
    "    for i in range(nb_pages):\n",
    "        if out_link_counts[i] > 0:\n",
    "            link_matrix[i] /= out_link_counts[i]\n",
    "\n",
    "    # Initialiser les PageRank\n",
    "    page_rank = np.ones(nb_pages) / nb_pages\n",
    "    damping_factor = 0.85\n",
    "    num_iterations = 100\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        page_rank = (1 - damping_factor) / nb_pages + damping_factor * link_matrix.T.dot(page_rank)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    page_rank_urls = [(urls[index], page_rank[index]) for index in range(nb_pages)]\n",
    "    page_rank_urls.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculer le score combiné TF-IDF + PageRank\n",
    "    cursor.execute(\"SELECT mot FROM index_mot ORDER BY mot DESC\")\n",
    "    mots = cursor.fetchall()\n",
    "\n",
    "    for mot in mots:\n",
    "        print(mot[0])\n",
    "        cursor.execute(\"SELECT occurences FROM index_mot WHERE mot = %s\", (mot[0],))\n",
    "        occurences = cursor.fetchone()[0]\n",
    "\n",
    "        if occurences:\n",
    "            pattern = re.compile(r'\\((\\d+),([^\\)]+)\\)')\n",
    "            matches = pattern.findall(occurences)\n",
    "            matches = [(int(match[0]), match[1].replace('\\\\', '').replace('\"', '').replace(' ', '')) for match in matches]\n",
    "\n",
    "            # Calcul du IDF\n",
    "            idf = np.log(nb_pages / len(matches)) if len(matches) > 0 else 0\n",
    "\n",
    "            # Calcul du TF-IDF et du PageRank\n",
    "            liste_url = []\n",
    "            for count, url in matches:\n",
    "                if url in urls.values():\n",
    "                    index = list(urls.values()).index(url)\n",
    "                    cursor.execute(\"SELECT nombre_mots FROM pages WHERE url = %s\", (url,))\n",
    "                    nb_mots = cursor.fetchone()[0]\n",
    "                    tf = count / nb_mots\n",
    "                    tf_idf = tf * idf\n",
    "                    page_rank_score = page_rank[index]\n",
    "                    combined_score = 0.7*tf_idf + 0.3 * page_rank_score\n",
    "                    liste_url.append((url, combined_score))\n",
    "\n",
    "            liste_url.sort(key=lambda x: x[1], reverse=True)\n",
    "            for url in liste_url:\n",
    "                print(url[0], url[1])\n",
    "\n",
    "            print(\"La page la plus pertinente est :\", liste_url[0][0])\n",
    "            print()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Erreur PostgreSQL:\", e)\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
